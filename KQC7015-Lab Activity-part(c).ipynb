{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75b8d2d3-b745-4cd1-ac8f-8b9edb7f70cd",
   "metadata": {},
   "source": [
    "# Least Mean Square (LMS) Algorithm\n",
    "\n",
    "The question (c) asks to implement the Least Mean Square (LMS) training rule to minimize the error function:\n",
    "\n",
    "### Steps:\n",
    "1. Calculate the error between predicted and target outputs.\n",
    "2. Use gradient descent to update the model's weights and bias.\n",
    "3. Iteratively optimize the parameters to reduce the error function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1f5fa39e-e3a0-481c-85ef-65d9372b805b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a5f72a-4a33-4b72-bada-8d89f28cda7a",
   "metadata": {},
   "source": [
    "### 1. Generate Synthetic Data\n",
    "- Objective: Simulate a dataset for training, based on a linear model with added noise.\n",
    "- Mathematical Formulation:\n",
    "  $$\n",
    "    y = w_1x_1 + w_2x_2 + \\dots + w_nx_n + b + \\epsilon\n",
    "  $$\n",
    "  - $( w_1, w_2, \\dots, w_n )$: True weights (randomly generated for simulation).\n",
    "  - $( b )$: True bias (constant value).\n",
    "  - $( \\epsilon )$: Random noise added to simulate real-world imperfections.\n",
    "- Note: The number of features (`n_features`) can be adjusted to test the LMS algorithm's flexibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "94739e98-2ba2-4a9d-ad41-e98820b36b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)                                # For reproducibility\n",
    "n_samples = 100                                  # Number of samples\n",
    "n_features = 5                                   # Number of features(Can be modified to any number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b69e150-d386-4b3d-986b-32c4559c157a",
   "metadata": {},
   "source": [
    "### Randomly generate input features and target outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d9b4d937-ef8f-4d61-9203-17d855fab89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(n_samples, n_features)        # Feature matrix\n",
    "true_weights = np.random.rand(n_features) * 10   # True weights\n",
    "true_bias = 5                                    # True bias\n",
    "noise = np.random.randn(n_samples) * 0.05        # Add noise\n",
    "y = np.dot(X, true_weights) + true_bias + noise  # Target outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d82e74-2f67-4460-8f66-162844277e8e",
   "metadata": {},
   "source": [
    "### 2. Initialize Weights and Bias\n",
    "- Initial weights (`w`) and bias (`b`) are randomly generated to avoid pre-existing bias in optimization.\n",
    "- Parameters:\n",
    "  - `learning_rate`: Controls the step size in gradient descent. A smaller value ensures stability but slows convergence.\n",
    "  - `num_epochs`: Defines the number of iterations for the optimization process.\n",
    "- Importance: Proper initialization ensures that the optimization process begins effectively without introducing systematic errors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7ced4162-b0ac-4ce8-ad94-6f0d3166b95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.randn(n_features)                  # Random initialization of weights\n",
    "b = np.random.randn()                            # Random initialization of bias\n",
    "learning_rate = 0.005                            # Learning rate\n",
    "num_epochs = 5000                                # Number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96022867-a37e-43eb-a49c-56c50c96fe78",
   "metadata": {},
   "source": [
    "### 3. LMS Training Loop\n",
    "\n",
    "#### Core Steps:\n",
    "\n",
    "1. **Prediction**:  \n",
    "   Compute predicted output $( y_{\\text{pred}} )$ using the current weights and bias:  \n",
    "   $$\n",
    "   y_{\\text{pred}} = w \\cdot x + b\n",
    "   $$\n",
    "\n",
    "2. **Error Calculation**:  \n",
    "   Compute the error between predictions and target values:  \n",
    "   $$\n",
    "   \\text{error} = y_{\\text{pred}} - y\n",
    "   $$\n",
    "\n",
    "3. **Gradient Calculation**:  \n",
    "   Calculate gradients for weights and bias to determine their adjustment directions:  \n",
    "\n",
    "   - For weights:  \n",
    "     $$\n",
    "     \\frac{\\partial E}{\\partial w_i} = \\frac{1}{N} \\sum_{d=1}^N (o_d - t_d) \\cdot x_{i,d}\n",
    "     $$\n",
    "   - For bias:  \n",
    "     $$\n",
    "     \\frac{\\partial E}{\\partial b} = \\frac{1}{N} \\sum_{d=1}^N (o_d - t_d)\n",
    "     $$\n",
    "\n",
    "4. **Parameter Update**:  \n",
    "   Update weights and bias using gradient descent:  \n",
    "\n",
    "   - For weights:  \n",
    "     $$\n",
    "     w_i^{\\text{new}} = w_i^{\\text{old}} - \\eta \\cdot \\frac{\\partial E}{\\partial w_i}\n",
    "     $$\n",
    "   - For bias:  \n",
    "     $$\n",
    "     b^{\\text{new}} = b^{\\text{old}} - \\eta \\cdot \\frac{\\partial E}{\\partial b}\n",
    "     $$\n",
    "\n",
    "5. **Monitor Loss**:  \n",
    "   Use Mean Squared Error (MSE) to monitor optimization progress:  \n",
    "   $$\n",
    "   \\text{Cost} = \\frac{1}{N} \\sum_{d=1}^N (\\text{error})^2\n",
    "   $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ffff2548-094a-4bd7-a7f4-f281d9c8cef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Cost: 322.2170845354234\n",
      "Epoch 100, Cost: 33.19197162802473\n",
      "Epoch 200, Cost: 4.490148618214638\n",
      "Epoch 300, Cost: 1.5712811708468035\n",
      "Epoch 400, Cost: 1.2105162499715398\n",
      "Epoch 500, Cost: 1.1077591479586153\n",
      "Epoch 600, Cost: 1.0349353382851953\n",
      "Epoch 700, Cost: 0.9691662267767933\n",
      "Epoch 800, Cost: 0.9079122510746837\n",
      "Epoch 900, Cost: 0.8506666804119295\n",
      "Epoch 1000, Cost: 0.7971415852058235\n",
      "Epoch 1100, Cost: 0.7470869784679774\n",
      "Epoch 1200, Cost: 0.7002717646957447\n",
      "Epoch 1300, Cost: 0.6564807932951436\n",
      "Epoch 1400, Cost: 0.6155135827897876\n",
      "Epoch 1500, Cost: 0.5771832809732026\n",
      "Epoch 1600, Cost: 0.5413157137510696\n",
      "Epoch 1700, Cost: 0.5077485033072047\n",
      "Epoch 1800, Cost: 0.47633024924937806\n",
      "Epoch 1900, Cost: 0.4469197680117825\n",
      "Epoch 2000, Cost: 0.41938538625731697\n",
      "Epoch 2100, Cost: 0.3936042843515667\n",
      "Epoch 2200, Cost: 0.3694618862738697\n",
      "Epoch 2300, Cost: 0.346851292601082\n",
      "Epoch 2400, Cost: 0.32567275344914715\n",
      "Epoch 2500, Cost: 0.3058331784882651\n",
      "Epoch 2600, Cost: 0.28724568136051315\n",
      "Epoch 2700, Cost: 0.269829156025893\n",
      "Epoch 2800, Cost: 0.2535078827448766\n",
      "Epoch 2900, Cost: 0.23821116157400582\n",
      "Epoch 3000, Cost: 0.22387297140686838\n",
      "Epoch 3100, Cost: 0.2104316527368497\n",
      "Epoch 3200, Cost: 0.1978296124513819\n",
      "Epoch 3300, Cost: 0.18601304909067398\n",
      "Epoch 3400, Cost: 0.17493169711808224\n",
      "Epoch 3500, Cost: 0.16453858885486747\n",
      "Epoch 3600, Cost: 0.15478983282981734\n",
      "Epoch 3700, Cost: 0.14564440738474416\n",
      "Epoch 3800, Cost: 0.13706396846062796\n",
      "Epoch 3900, Cost: 0.12901267056678312\n",
      "Epoch 4000, Cost: 0.12145700000726892\n",
      "Epoch 4100, Cost: 0.11436561950533206\n",
      "Epoch 4200, Cost: 0.10770922342833247\n",
      "Epoch 4300, Cost: 0.10146040287274272\n",
      "Epoch 4400, Cost: 0.09559351992173419\n",
      "Epoch 4500, Cost: 0.09008459043696299\n",
      "Epoch 4600, Cost: 0.08491117479162272\n",
      "Epoch 4700, Cost: 0.080052275994029\n",
      "Epoch 4800, Cost: 0.07548824469007082\n",
      "Epoch 4900, Cost: 0.07120069056913579\n",
      "Epoch 4999, Cost: 0.06721145307401767\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    y_pred = np.dot(X, w) + b                          # Calculate prediction\n",
    "    error = y_pred - y                                 # Calculate error\n",
    "    grad_w = np.dot(X.T, error) / n_samples            # Gradient for weights\n",
    "    grad_b = np.sum(error) / n_samples                 # Gradient for bias\n",
    "    w -= learning_rate * grad_w                        # Update weights and bias\n",
    "    b -= learning_rate * grad_b\n",
    "    if epoch % 100 == 0 or epoch == num_epochs - 1:\n",
    "        cost = np.mean(error ** 2)\n",
    "        print(f\"Epoch {epoch}, Cost: {cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba45187-cc55-4b28-8ddf-83fff07d2ef1",
   "metadata": {},
   "source": [
    "**Cost Analysis**:\n",
    "   - The initial cost value was `322.217`, indicating a significant gap between the model's initial predictions and the true values.\n",
    "   - As iterations progressed, the cost decreased rapidly, eventually converging to `0.0672` at the `5000th` iteration.\n",
    "   - This trend demonstrates that the LMS algorithm effectively optimized the model, allowing it to fit the data well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d6b216-ffdd-4212-9639-f54d79a83902",
   "metadata": {},
   "source": [
    "### 4. Output Final Results\n",
    "- The final weights (`w`) and bias (`b`) are compared against the true values used in data generation.\n",
    "- Verification:\n",
    "  - If the final values are close to the true values, it confirms that the LMS algorithm effectively optimized the parameters.\n",
    "  - A low final cost indicates successful convergence.\n",
    "- Next Steps:\n",
    "  - If results deviate significantly, adjust parameters such as the learning rate or the number of epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0dfe83af-0bf4-47b4-8b33-2a7147efb2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Weights: [3.13161192 3.46230439 4.65005865 7.04404492 3.3471348 ]\n",
      "Final Bias: 5.679740146109735\n",
      "\n",
      "True Weights: [3.10380826 3.73034864 5.24970442 7.50595023 3.33507466]\n",
      "True Bias: 5\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFinal Weights:\", w)\n",
    "print(\"Final Bias:\", b)\n",
    "print(\"\\nTrue Weights:\", true_weights)\n",
    "print(\"True Bias:\", true_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cdd3c1-996e-4de5-8526-de3a01e42b89",
   "metadata": {},
   "source": [
    "**Weights and Bias Fitting Performance**:\n",
    "   - The final weights (`w`) showed a relative error of less than 12% compared to the true weights (`true_weights`), with most dimensions having an error below 5%.\n",
    "   - The final bias (`b`) had an absolute error of `0.6797` and a relative error of `13.59%`, which is still within a reasonable range.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8330e0-d7f6-49e0-aac1-7120fe967111",
   "metadata": {},
   "source": [
    "### **Summary and Conclusion**\n",
    "\n",
    "1. **Effectiveness of the LMS Algorithm**:\n",
    "   - The LMS algorithm successfully optimized the model parameters through gradient descent, resulting in weights and bias values that are highly close to the true values after convergence of the loss function.\n",
    "   - In this experiment, the final loss value was `0.0672`, indicating minimal error and demonstrating that the model fit the data effectively.\n",
    "\n",
    "2. **Impact of Noise on Results**:\n",
    "   - Despite the addition of noise to the data, the model was able to converge to parameters close to the true values, highlighting the robustness of the LMS algorithm against noise.\n",
    "\n",
    "3. **Room for Improvement**:\n",
    "   - To achieve even higher precision, adjustments such as reducing the learning rate or increasing the number of iterations could be made. However, the current results already meet the experimental requirements satisfactorily.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
